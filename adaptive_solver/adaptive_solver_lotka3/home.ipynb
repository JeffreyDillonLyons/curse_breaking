{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import chaospy as ch\n",
    "from scipy import integrate\n",
    "import pandas as pd\n",
    "import time\n",
    "import json\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import numpoly\n",
    "from itertools import product\n",
    "from collections import OrderedDict\n",
    "import re\n",
    "import os\n",
    "import matplotlib as mpl\n",
    "\n",
    "\n",
    "def c(s):\n",
    "    os.chdir(s)\n",
    "    return os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "'''Preamble'''\n",
    "\n",
    "import numpy as np\n",
    "import chaospy as ch\n",
    "from scipy import integrate\n",
    "import pandas as pd\n",
    "import time\n",
    "import json\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import numpoly\n",
    "from itertools import product\n",
    "from collections import OrderedDict\n",
    "import re\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def c(s):\n",
    "    os.chdir(s)\n",
    "    return os.getcwd()\n",
    "''''''\n",
    "\n",
    "'''\n",
    " Notes\n",
    "-------------------------\n",
    "TODO > Remove indices in active that are in old before assigning errors > affects first step since 1111 already in old. \n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "x0 = 33                         # Initial conditions same as before\n",
    "y0 = 6.2\n",
    "X = [x0,y0]\n",
    "t = np.linspace(0., 30, 1000)\n",
    "\n",
    "#Model\n",
    "\n",
    "def lotka(X, t, alpha, beta, delta, gamma):\n",
    "    x, y = X\n",
    "    dotx = x * (alpha - beta * y)\n",
    "    doty = y * (-delta + gamma * x)\n",
    "    return np.array([dotx, doty])\n",
    "\n",
    "#Parameter space\n",
    "\n",
    "problem = {\n",
    "    'num_vars': 4,\n",
    "    'names': ['alpha','beta','delta','gamma'],\n",
    "    'bounds': [[0.44,0.68],\n",
    "               [0.02,0.044],\n",
    "               [0.71,1.15],\n",
    "               [0.0226,0.0354]]\n",
    "}\n",
    "##Parameters\n",
    "alpha = ch.Uniform(0.44, 0.68) #We choose uniform distributions to reflect our lack of knowledge about the relative likelihood functions\n",
    "beta = ch.Uniform(0.02, 0.044) #We take the same bounds as for the Sobol-Saltelli analysis\n",
    "delta = ch.Uniform(0.71, 1.15)\n",
    "gamma = ch.Uniform(0.0226, 0.0354)\n",
    "\n",
    "joint = ch.J(alpha,beta,delta,gamma) #The input paramter distributions are assumed to be independent so we may easily construct the joint input probability distribution.\n",
    "\n",
    "\n",
    "gt_prey = pd.read_csv('../data/new_gt_indices_328000.csv',index_col=['type','params']).loc['pred','ST']\n",
    "gt_norm = np.linalg.norm(gt_prey)\n",
    "\n",
    "\n",
    "dick = {}\n",
    "\n",
    "vectors = np.identity(len(joint), dtype='int')\n",
    "\n",
    "growth=False\n",
    "recurrence_algorithm='stieltjes'\n",
    "rule='g'\n",
    "tolerance=1e-10\n",
    "scaling=3\n",
    "n_max=50000\n",
    "\n",
    "\n",
    "def _construct_lookup(\n",
    "        orders,\n",
    "        dists,\n",
    "        growth,\n",
    "        recurrence_algorithm,\n",
    "        rules,\n",
    "        tolerance,\n",
    "        scaling,\n",
    "        n_max,\n",
    "):\n",
    "    \"\"\"\n",
    "    Create abscissas and weights look-up table so values do not need to be\n",
    "    re-calculatated on the fly.\n",
    "    \"\"\"\n",
    "    x_lookup = []\n",
    "    w_lookup = []\n",
    "\n",
    "    for order, dist in zip(max_order_vector, dists):\n",
    "        x_lookup.append([])\n",
    "        w_lookup.append([])\n",
    "        for orderr in range(max_order + 1):\n",
    "            (abscissas,), weights = ch.generate_quadrature(\n",
    "                order=orderr,\n",
    "                dist=dist,\n",
    "                growth=growth,\n",
    "                recurrence_algorithm=recurrence_algorithm,\n",
    "                rule=rule,\n",
    "                tolerance=tolerance,\n",
    "                scaling=scaling,\n",
    "                n_max=n_max,\n",
    "            )\n",
    "            x_lookup[-1].append(abscissas)\n",
    "            w_lookup[-1].append(weights)\n",
    "    return x_lookup, w_lookup\n",
    "\n",
    "\n",
    "def construct_wrapper(maxx):\n",
    "    global max_order_vector, max_order\n",
    "    global x_lookup, w_lookup\n",
    "\n",
    "    max_order = maxx\n",
    "    max_order_vector = max_order * np.ones(len(joint), dtype=int)\n",
    "    # print(max_order_vector)\n",
    "\n",
    "    x_lookup, w_lookup = _construct_lookup(\n",
    "        orders=max_order_vector,\n",
    "        dists=joint,\n",
    "        growth=growth,\n",
    "        recurrence_algorithm=recurrence_algorithm,\n",
    "        rules=rule,\n",
    "        tolerance=tolerance,\n",
    "        scaling=scaling,\n",
    "        n_max=5000)\n",
    "\n",
    "    return x_lookup, w_lookup\n",
    "\n",
    "x_lookup, w_lookup = construct_wrapper(10)\n",
    "\n",
    "def generate_candidates(index_set, P):\n",
    "    global candidates, pre_candidates, back_neighbours\n",
    "\n",
    "    pre_candidates = []\n",
    "    candidates = []\n",
    "\n",
    "    for j in range(0, len(joint)):\n",
    "        pre_candidates.append(index_set + vectors[j])\n",
    "\n",
    "    for candidate in pre_candidates:\n",
    "        back_neighbours = []\n",
    "        for j in range(0, len(joint)):\n",
    "            back_neighbour = candidate - vectors[j]\n",
    "            if np.all((back_neighbour > 1)):\n",
    "                back_neighbours.append(tuple(back_neighbour))\n",
    "\n",
    "        if np.all([neighbour in old for neighbour in back_neighbours]):\n",
    "            candidates.append(tuple(candidate))\n",
    "\n",
    "    temp = []\n",
    "    for candidate in candidates:\n",
    "        # if candidate not in old:\n",
    "        if (np.all(np.array(candidate) <= P) and np.linalg.norm(np.array(candidate),ord=1) <= (P+3)):\n",
    "            temp.append(candidate)\n",
    "\n",
    "    candidates = temp\n",
    "    \n",
    "    for candidate in candidates:\n",
    "        if candidate in old:\n",
    "            candidates.remove(candidate)\n",
    "\n",
    "#     temp = []\n",
    "#     maxx = sum(np.max(np.array(old), axis=0))\n",
    "\n",
    "#     for candidate in candidates:\n",
    "\n",
    "#         if sum(np.max(np.array(old + [candidate]), axis=0)) > maxx:\n",
    "#             temp.append(candidate)\n",
    "\n",
    "#     candidates = temp\n",
    "    return candidates\n",
    "\n",
    "def sobol_error(vec):\n",
    "    return np.linalg.norm(gt_prey - vec) / gt_norm\n",
    "\n",
    "def solver(old_set,target):\n",
    "\n",
    "    global poly\n",
    "\n",
    "    solver_time = time.perf_counter()\n",
    "\n",
    "    nodes_list = []\n",
    "    weights_list = []\n",
    "    evals_list = []\n",
    "\n",
    "    for index in old_set:\n",
    "        nodes, weights = build_nodes_weights(index)\n",
    "        weights = [weight * combinator(index) for weight in weights]\n",
    "\n",
    "        nodes_list += nodes\n",
    "        weights_list += weights\n",
    "\n",
    "    for node in nodes_list:\n",
    "\n",
    "        if node in dick.keys():\n",
    "            evals_list.append(dick[node])\n",
    "\n",
    "        else:\n",
    "            a, b, d, g = node\n",
    "            solution = integrate.odeint(lotka, X, t, args=(a, b, d, g)).T[target][910]\n",
    "            evals_list.append(solution)\n",
    "            dick[node] = solution\n",
    "\n",
    "    nodes_list = np.array(nodes_list).T\n",
    "\n",
    "    polly, uhat = ch.fit_regression(expansion,nodes_list, evals_list, retall = 1)\n",
    "    poly.append(polly)\n",
    "    # print('Solver_time >>>', time.perf_counter() - solver_time)\n",
    "    return len(weights_list), uhat\n",
    "\n",
    "def assign_errors(active_set):\n",
    "    global active_errors, active, candidates, current_errors,new\n",
    "    \n",
    "    active_errors = []\n",
    "    \n",
    "    if np.any(active_set in old):\n",
    "        print('oops')\n",
    "    \n",
    "#     maxx = sum(np.max(np.array(old), axis=0))\n",
    "\n",
    "#     for multi_index in active_set:\n",
    "#         if (step > 0) and sum(np.max(np.array(old + [multi_index]), axis=0)) <= maxx:\n",
    "#             active_set.remove(multi_index)\n",
    "   \n",
    "\n",
    "    for multi_index in active_set:\n",
    "        nodes, _ = build_nodes_weights(multi_index)\n",
    "        current_errors = []\n",
    "\n",
    "        for node in nodes:\n",
    "            a, b, d, g = node\n",
    "            if np.isnan(poly[-1](a, b, d, g)):\n",
    "                poly_eval = 0\n",
    "            else:\n",
    "                poly_eval = poly[-1](a, b, d, g)\n",
    "\n",
    "            if node in dick.keys():\n",
    "                a, b, d, g = node\n",
    "                current_errors.append(abs(dick[node] - poly_eval))\n",
    "\n",
    "            else:\n",
    "                solution = integrate.odeint(lotka, (33, 6.2), t, args=(a, b, d, g)).T[0][910]\n",
    "                current_errors.append(abs(solution - poly_eval))\n",
    "\n",
    "        active_errors.append(np.mean(current_errors))\n",
    "\n",
    "    active = sorted(list(zip(active_set, active_errors)), key=lambda x: x[1])\n",
    "    for i in active:\n",
    "        if step > 1:\n",
    "            if i[0] in old:\n",
    "                active.remove(i)\n",
    "       \n",
    "\n",
    "    # print(active)\n",
    "    return active\n",
    "    # active = [i for i in OrderedDict((tuple(x[0]), x) for x in active).values()]if np.isnan(poly[-1](1,2,3,4)):\n",
    "\n",
    "def algorithm(P,species,TOL, merge):\n",
    "    \n",
    "    global dick, old, candidates, poly, active, global_errors, no_nodes,step,expansion,means,uhats\n",
    "    \n",
    "    '''Initialise'''\n",
    "    \n",
    "    if species == 'prey':\n",
    "        target = 0\n",
    "        \n",
    "    elif species == 'predator':\n",
    "        target = 1\n",
    "    \n",
    "    seed = (2,2,1,2)\n",
    "    expansion = ch.generate_expansion(P, joint, normed = True)\n",
    "    exponents = ch.lead_exponent(expansion, graded=True)\n",
    "    vectors = np.identity(len(joint), dtype='int')\n",
    "    date_today = datetime.date.today()\n",
    "    start_time = time.perf_counter()\n",
    "    \n",
    "    step = 0\n",
    "    \n",
    "    old = [(1,1,1,1)]\n",
    "    active = []\n",
    "    poly = []\n",
    "    uhats = []\n",
    "    \n",
    "    local_errors = []\n",
    "    global_errors = []\n",
    "    means = []\n",
    "    \n",
    "    names = ['alpha','beta','delta','gamma']\n",
    "    \n",
    "    df = pd.DataFrame(columns=['chosen_index','local_error','global_error','no_nodes','run_time'],dtype=object)\n",
    "    df_indices = pd.DataFrame(columns=['alpha','beta','delta','gamma'],dtype=object)\n",
    "    df_indices_s1 = pd.DataFrame(columns=['alpha','beta','delta','gamma'],dtype=object)\n",
    "\n",
    "    \n",
    "    '''Execute zeroth step'''\n",
    "    \n",
    "    trivial = [seed]\n",
    "    number_nodes,uhat = solver(old,target)\n",
    "    uhats.append(uhat)\n",
    "    assign_errors(old)\n",
    "    \n",
    "    \n",
    "    st = sense_t(uhat,exponents)\n",
    "    s1 = sense_main(uhat,exponents)\n",
    "    means.append(uhat[0])\n",
    "    # print(uhat[0])\n",
    "    \n",
    "    global_errors.append(sobol_error(st))\n",
    "    \n",
    "    print('Global error >>>', global_errors[-1])\n",
    "    print('Step time >>>', time.perf_counter() - start_time, 'seconds')\n",
    "    print('-'*10,'break','-'*10)\n",
    "\n",
    "\n",
    "    '''Main loop'''\n",
    "    \n",
    "    while (global_errors[-1] > TOL or np.isnan(global_errors[-1])) and len(active)>0:\n",
    "        \n",
    "        start_time = time.perf_counter()\n",
    "        \n",
    "#         print('Active >>>',active)\n",
    "        chosen_index = active[-1][0]\n",
    "        local_errors.append(active[-1][1])\n",
    "        active.pop()\n",
    "    \n",
    "        old.append(chosen_index)\n",
    "        \n",
    "        print('Chosen index >>>', chosen_index)\n",
    "        \n",
    "        number_nodes,uhat = solver(old,target)\n",
    "        uhats.append(uhat)\n",
    "        \n",
    "        candidates = generate_candidates(chosen_index,P)\n",
    "        stripped_active = [i[0] for i in active] + [j for j in candidates]\n",
    "        active = assign_errors(stripped_active)\n",
    "        \n",
    "        sobol_time = time.perf_counter() \n",
    "\n",
    "        st = sense_t(uhat,exponents)\n",
    "        s1 = sense_main(uhat,exponents)\n",
    "        means.append(uhat[0])\n",
    "        print(uhat[0])\n",
    "\n",
    "        \n",
    "        # print('Sobol time >>>', time.perf_counter() - sobol_time)\n",
    "        \n",
    "        global_errors.append(sobol_error(st))\n",
    "\n",
    "        print('Global error >>>', global_errors[-1])\n",
    "        \n",
    "        '''Save data'''\n",
    "        run_time = time.perf_counter() - start_time\n",
    "        \n",
    "        numpoly.savez(f'../data/lotka2/{species}/poly_{P}+{date_today}.npz',*poly)\n",
    "        np.savez(f'../data/lotka2/{species}/uhat_{P}+{date_today}.npz',*uhats)\n",
    "        \n",
    "        df_indices = df_indices.append({'alpha': st[0], 'beta': st[1], 'delta': st[2], 'gamma': st[3]          }, ignore_index=True)\n",
    "        df_indices_s1 = df_indices_s1.append({'alpha': s1[0], 'beta': s1[1], 'delta': s1[2], 'gamma': s1[3]          }, ignore_index=True)\n",
    "        df = df.append({'chosen_index': chosen_index,'local_error':local_errors[-1],                               'global_error':global_errors[-1],'no_nodes':number_nodes, 'run_time':run_time}, ignore_index=True)\n",
    "        \n",
    "        df.to_csv(f'../data/lotka2/{species}/run_file_{P}+{date_today}.csv')\n",
    "        df_indices.to_csv(f'../data/lotka2/{species}/total_order_indices_{P}+{date_today}.csv')\n",
    "        df_indices_s1.to_csv(f'../data/lotka2/{species}/first_order_indices_{P}+{date_today}.csv')\n",
    "\n",
    "        print('Step time >>>', time.perf_counter() - start_time, 'seconds')\n",
    "        print('-'*10,'break','-'*10)\n",
    "        \n",
    "        step += 1\n",
    "        \n",
    "    print('Congratulations, the algorithm has converged!')\n",
    "    print('Here are the results...')\n",
    "    print('-'*20)\n",
    "    print(f'ST_alpha:{st[0].round(10)}, ST_beta:{st[1].round(10)}, ST_delta:{st[2].round(10)}, ST_gamma:{st[3].round(10)}')\n",
    "    print(f'GT_alpha:{gt_prey[0].round(10)}, GT_beta:{gt_prey[1].round(10)}, GT_delta:{gt_prey[2].round(10)}, GT_gamma:{gt_prey[3].round(10)}')\n",
    "    print(f'The final grid contains {number_nodes} nodes.')\n",
    "    print(f'The total run time was {df.run_time.sum()}seconds, not bad!')\n",
    "    \n",
    "    if merge:\n",
    "    \n",
    "        merged_set = merge_sets(old,active)\n",
    "        \n",
    "        number_nodes,uhat = solver(merged_set, target)\n",
    "            \n",
    "        st = sense_t(uhat,exponents)\n",
    "        s1 = sense_main(uhat,exponents)\n",
    "        \n",
    "        print('-'*10,'MERGED','-'*10)\n",
    "        print(f'ST_alpha:{st[0].round(10)}, ST_beta:{st[1].round(10)}, ST_delta:{st[2].round(10)}, ST_gamma:{st[3].round(10)}')\n",
    "        print(f'GT_alpha:{gt_prey[0].round(10)}, GT_beta:{gt_prey[1].round(10)}, GT_delta:{gt_prey[2].round(10)}, GT_gamma:{gt_prey[3].round(10)}')\n",
    "        \n",
    "        global_errors.append(sobol_error(st))\n",
    "\n",
    "        print('Global error >>>', global_errors[-1])\n",
    "            \n",
    "        '''Save data'''\n",
    "        run_time = time.perf_counter() - start_time\n",
    "            \n",
    "        numpoly.savez(f'../data/lotka2/{species}/poly_{P}+{date_today}.npz',*poly)\n",
    "            \n",
    "        df_indices = df_indices.append({'alpha': st[0], 'beta': st[1], 'delta': st[2], 'gamma': st[3]          }, ignore_index=True)\n",
    "        df_indices_s1 = df_indices_s1.append({'alpha': s1[0], 'beta': s1[1], 'delta': s1[2], 'gamma': s1[3]          }, ignore_index=True)\n",
    "        df = df.append({'chosen_index': chosen_index,'local_error':local_errors[-1],                               'global_error':global_errors[-1],'no_nodes':number_nodes, 'run_time':run_time}, ignore_index=True)\n",
    "            \n",
    "        df.to_csv(f'../data/lotka2/{species}/run_file_{P}+{date_today}.csv')\n",
    "        df_indices.to_csv(f'../data/lotka2/{species}/total_order_indices_{P}+{date_today}.csv')\n",
    "        df_indices_s1.to_csv(f'../data/lotka2/{species}/first_order_indices_{P}+{date_today}.csv')\n",
    "        \n",
    "    print(old)\n",
    "         \n",
    "def merge_sets(old_set,active):\n",
    "\n",
    "    stripped_active = [i[0] for i in active]\n",
    "    merged = old + stripped_active\n",
    "    \n",
    "    return merged\n",
    "    \n",
    "def plot_stat_convergence(means):\n",
    "       \n",
    "        K = len(means)\n",
    "        if K < 2:\n",
    "            print('Means from at least two refinements are required')\n",
    "            return\n",
    "        else:\n",
    "            differ_mean = np.zeros(K - 1)\n",
    "            differ_std = np.zeros(K - 1)\n",
    "            for i in range(1, K):\n",
    "                differ_mean[i - 1] = means[i] - means[i - 1]\n",
    "                                                    \n",
    "                # make relative\n",
    "                differ_mean[i - 1] = differ_mean[i - 1] / means[i - 1]\n",
    "                                                                         \n",
    "\n",
    "                # differ_std[i - 1] = np.linalg.norm(self.std_history[i] -\n",
    "                                                   # self.std_history[i - 1], np.inf)\n",
    "                # # make relative\n",
    "                # differ_std[i - 1] = differ_std[i - 1] / np.linalg.norm(self.std_history[i - 1],\n",
    "                                                                       # np.inf)\n",
    "\n",
    "        import matplotlib.pyplot as plt\n",
    "        fig = plt.figure('stat_conv')\n",
    "        ax1 = fig.add_subplot(111, title='moment convergence')\n",
    "        ax1.set_xlabel('iteration', fontsize=12)\n",
    "        # ax1.set_ylabel(r'$ ||\\mathrm{mean}_i - \\mathrm{mean}_{i - 1}||_\\infty$',\n",
    "        # color='r', fontsize=12)\n",
    "        ax1.set_ylabel(r'relative error mean', color='r', fontsize=12)\n",
    "        ax1.plot(range(2, K + 1), differ_mean, color='r', marker='+')\n",
    "        ax1.tick_params(axis='y', labelcolor='r')\n",
    "        plt.show()\n",
    "def combinator(current_index):\n",
    "    \n",
    "    coeff = 1\n",
    "    \n",
    "    for vector in vectors:\n",
    "        \n",
    "        if tuple(np.array(current_index, dtype='int') + vector) in old:\n",
    "            \n",
    "            coeff += -1\n",
    "            \n",
    "    return coeff \n",
    "\n",
    "def build_nodes_weights(current_index):\n",
    "    \n",
    "    nodestack = []\n",
    "    weightstack = []\n",
    "    \n",
    "    '''Nodes'''\n",
    "    \n",
    "    for index,element in enumerate(current_index):\n",
    "        nodestack.append([])\n",
    "        nodestack[index] = list(x_lookup[index][element])\n",
    "        \n",
    "    nodes = nodestack[0]\n",
    "    \n",
    "    for i in range(1,len(nodestack)):\n",
    "        nodes = product(nodes,nodestack[i])\n",
    "        \n",
    "    nodes = [(a,b,c,d) for (((a,b),c),d) in nodes]\n",
    "    \n",
    "    '''Weights'''\n",
    "    \n",
    "    for index,element in enumerate(current_index):\n",
    "        weightstack.append([])\n",
    "        weightstack[index] = list(w_lookup[index][element])\n",
    "        \n",
    "    weights = weightstack[0]\n",
    "    \n",
    "    for i in range(1,len(weightstack)):\n",
    "        weights = product(weights,weightstack[i])\n",
    "        \n",
    "    weights = [(a*b*c*d) for (((a,b),c),d) in weights]\n",
    "    \n",
    "    \n",
    "    return nodes,weights\n",
    "\n",
    "def sense_main(uhat,exponents):\n",
    "\n",
    "    dim = len(joint)\n",
    "    s1 = np.zeros(dim)\n",
    "    \n",
    "    variance = np.sum(np.array(uhat[1:])**2)\n",
    "    \n",
    "    for variable,name in enumerate(expansion.names):\n",
    "        mask = np.ones(dim)\n",
    "        mask[variable] = False\n",
    "        \n",
    "        for idx,exponent in enumerate(exponents):\n",
    "            if exponent[variable] > 0 and np.all(exponent*mask == 0):\n",
    "                s1[variable] += uhat[idx]**2\n",
    "                \n",
    "    s1 = s1 / variance\n",
    "    \n",
    "    return s1\n",
    "\n",
    "def sense_t(uhat,exponents):\n",
    "    \n",
    "    dim = len(joint)\n",
    "    st = np.zeros(dim)\n",
    "    \n",
    "    variance = np.sum(np.array(uhat[1:])**2)\n",
    "    \n",
    "    for variable,name in enumerate(expansion.names):\n",
    "\n",
    "        mask = np.ones(dim)\n",
    "        mask[variable] = False\n",
    "        \n",
    "        for idx,exponent in enumerate(exponents):\n",
    "            if exponent[variable] > 0 and np.all(exponent*mask == 0):\n",
    "                st[variable] += uhat[idx]**2\n",
    "                \n",
    "            if exponent[variable] > 0 and np.any(exponent*mask != 0):\n",
    "                st[variable] += uhat[idx]**2\n",
    "                \n",
    "    st = st / variance\n",
    "    \n",
    "    return st\n",
    "         \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0)\n"
     ]
    }
   ],
   "source": [
    "print(tuple(np.ones(7)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes,_ = build_nodes_weights((1,1,1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tuple'>\n"
     ]
    }
   ],
   "source": [
    "print(type(nodes[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "params\n",
       "alpha    0.675247\n",
       "beta     0.762646\n",
       "delta    0.007383\n",
       "gamma    0.091910\n",
       "e        0.045190\n",
       "f        0.671680\n",
       "h        0.500192\n",
       "Name: ST, dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pd.read_csv('./data/gt_L3.csv',index_col=['type','params']).loc['owl','ST']\n",
    "test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
