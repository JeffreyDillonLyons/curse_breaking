{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './data/indices_328000_GT.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-aba6a51c5405>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m \u001b[0mgt_prey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./data/indices_328000_GT.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mST\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     60\u001b[0m \u001b[0mgt_norm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgt_prey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\gds\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    684\u001b[0m     )\n\u001b[0;32m    685\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 686\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    687\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    688\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\gds\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    450\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    451\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 452\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    453\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    454\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\gds\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    934\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    935\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 936\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    937\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    938\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\gds\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1166\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"c\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1167\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"c\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1168\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1169\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1170\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"python\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\gds\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1996\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"usecols\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1997\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1998\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1999\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2000\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './data/indices_328000_GT.csv'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "'''Preamble'''\n",
    "\n",
    "import numpy as np\n",
    "import chaospy as ch\n",
    "from scipy import integrate\n",
    "import pandas as pd\n",
    "import time\n",
    "import json\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import numpoly\n",
    "from itertools import product\n",
    "from collections import OrderedDict\n",
    "import re\n",
    "import os\n",
    "\n",
    "\n",
    "def c(s):\n",
    "    os.chdir(s)\n",
    "    return os.getcwd()\n",
    "''''''\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "x0 = 33                         # Initial conditions same as before\n",
    "y0 = 6.2\n",
    "X = [x0,y0]\n",
    "t = np.linspace(0., 30, 1000)\n",
    "\n",
    "#Model\n",
    "\n",
    "def lotka(X, t, alpha, beta, delta, gamma):\n",
    "    x, y = X\n",
    "    dotx = x * (alpha - beta * y)\n",
    "    doty = y * (-delta + gamma * x)\n",
    "    return np.array([dotx, doty])\n",
    "\n",
    "#Parameter space\n",
    "\n",
    "problem = {\n",
    "    'num_vars': 4,\n",
    "    'names': ['alpha','beta','delta','gamma'],\n",
    "    'bounds': [[0.44,0.68],\n",
    "               [0.02,0.044],\n",
    "               [0.71,1.15],\n",
    "               [0.0226,0.0354]]\n",
    "}\n",
    "##Parameters\n",
    "alpha = ch.Uniform(0.44, 0.68) #We choose uniform distributions to reflect our lack of knowledge about the relative likelihood functions\n",
    "beta = ch.Uniform(0.02, 0.044) #We take the same bounds as for the Sobol-Saltelli analysis\n",
    "delta = ch.Uniform(0.71, 1.15)\n",
    "gamma = ch.Uniform(0.0226, 0.0354)\n",
    "\n",
    "joint = ch.J(alpha,beta,delta,gamma) #The input paramter distributions are assumed to be independent so we may easily construct the joint input probability distribution.\n",
    "\n",
    "\n",
    "\n",
    "gt_prey = pd.read_csv('./data/indices_328000_GT.csv').ST\n",
    "gt_norm = np.linalg.norm(gt_prey)\n",
    "\n",
    "\n",
    "dick = {}\n",
    "\n",
    "vectors = np.identity(len(joint), dtype='int')\n",
    "\n",
    "growth=False\n",
    "recurrence_algorithm='stieltjes'\n",
    "rule='g'\n",
    "tolerance=1e-10\n",
    "scaling=3\n",
    "n_max=50000\n",
    "\n",
    "\n",
    "def _construct_lookup(\n",
    "        orders,\n",
    "        dists,\n",
    "        growth,\n",
    "        recurrence_algorithm,\n",
    "        rules,\n",
    "        tolerance,\n",
    "        scaling,\n",
    "        n_max,\n",
    "):\n",
    "    \"\"\"\n",
    "    Create abscissas and weights look-up table so values do not need to be\n",
    "    re-calculatated on the fly.\n",
    "    \"\"\"\n",
    "    x_lookup = []\n",
    "    w_lookup = []\n",
    "\n",
    "    for order, dist in zip(max_order_vector, dists):\n",
    "        x_lookup.append([])\n",
    "        w_lookup.append([])\n",
    "        for orderr in range(max_order + 1):\n",
    "            (abscissas,), weights = ch.generate_quadrature(\n",
    "                order=orderr,\n",
    "                dist=dist,\n",
    "                growth=growth,\n",
    "                recurrence_algorithm=recurrence_algorithm,\n",
    "                rule=rule,\n",
    "                tolerance=tolerance,\n",
    "                scaling=scaling,\n",
    "                n_max=n_max,\n",
    "            )\n",
    "            x_lookup[-1].append(abscissas)\n",
    "            w_lookup[-1].append(weights)\n",
    "    return x_lookup, w_lookup\n",
    "\n",
    "\n",
    "def construct_wrapper(maxx):\n",
    "    global max_order_vector, max_order\n",
    "    global x_lookup, w_lookup\n",
    "\n",
    "    max_order = maxx\n",
    "    max_order_vector = max_order * np.ones(len(joint), dtype=int)\n",
    "    # print(max_order_vector)\n",
    "\n",
    "    x_lookup, w_lookup = _construct_lookup(\n",
    "        orders=max_order_vector,\n",
    "        dists=joint,\n",
    "        growth=growth,\n",
    "        recurrence_algorithm=recurrence_algorithm,\n",
    "        rules=rule,\n",
    "        tolerance=tolerance,\n",
    "        scaling=scaling,\n",
    "        n_max=5000)\n",
    "\n",
    "    return x_lookup, w_lookup\n",
    "\n",
    "x_lookup, w_lookup = construct_wrapper(10)\n",
    "\n",
    "\n",
    "def generate_candidates(index_set, P):\n",
    "    global candidates, pre_candidates, back_neighbours\n",
    "\n",
    "    pre_candidates = []\n",
    "    candidates = []\n",
    "\n",
    "    for j in range(0, len(joint)):\n",
    "        pre_candidates.append(index_set + vectors[j])\n",
    "\n",
    "    for candidate in pre_candidates:\n",
    "        back_neighbours = []\n",
    "        for j in range(0, len(joint)):\n",
    "            back_neighbour = candidate - vectors[j]\n",
    "            if np.all((back_neighbour > 1)):\n",
    "                back_neighbours.append(tuple(back_neighbour))\n",
    "\n",
    "        if np.all([neighbour in old for neighbour in back_neighbours]):\n",
    "            candidates.append(tuple(candidate))\n",
    "\n",
    "    temp = []\n",
    "    for candidate in candidates:\n",
    "        # if candidate not in old:\n",
    "        if (np.all(np.array(candidate) <= P) and np.linalg.norm(np.array(candidate),ord=1) <= (P+3)):\n",
    "            temp.append(candidate)\n",
    "\n",
    "    candidates = temp\n",
    "\n",
    "    temp = []\n",
    "    maxx = sum(np.max(np.array(old), axis=0))\n",
    "\n",
    "    for candidate in candidates:\n",
    "\n",
    "        if sum(np.max(np.array(old + [candidate]), axis=0)) > maxx:\n",
    "            temp.append(candidate)\n",
    "\n",
    "    candidates = temp\n",
    "\n",
    "    return candidates\n",
    "\n",
    "def sobol_error(vec):\n",
    "    return np.linalg.norm(gt_prey - vec) / gt_norm\n",
    "\n",
    "\n",
    "def solver(old_set,target):\n",
    "\n",
    "    global poly\n",
    "\n",
    "    solver_time = time.perf_counter()\n",
    "\n",
    "    nodes_list = []\n",
    "    weights_list = []\n",
    "    evals_list = []\n",
    "\n",
    "    for index in old_set:\n",
    "        nodes, weights = build_nodes_weights(index)\n",
    "        weights = [weight * combinator(index) for weight in weights]\n",
    "\n",
    "        nodes_list += nodes\n",
    "        weights_list += weights\n",
    "\n",
    "    for node in nodes_list:\n",
    "\n",
    "        if node in dick.keys():\n",
    "            evals_list.append(dick[node])\n",
    "\n",
    "        else:\n",
    "            a, b, d, g = node\n",
    "            solution = integrate.odeint(lotka, X, t, args=(a, b, d, g)).T[target][910]\n",
    "            evals_list.append(solution)\n",
    "            dick[node] = solution\n",
    "\n",
    "    nodes_list = np.array(nodes_list).T\n",
    "\n",
    "    polly, uhat = ch.fit_quadrature(expansion, nodes_list, weights_list, evals_list, retall = 1)\n",
    "    poly.append(polly)\n",
    "    print('Solver_time >>>', time.perf_counter() - solver_time)\n",
    "    print('Weight sum >>>', sum(weights_list))\n",
    "#     print(len(uhat))\n",
    "    return len(weights_list), uhat\n",
    "\n",
    "\n",
    "def assign_errors(active_set):\n",
    "    global active_errors, active, candidates, current_errors, new\n",
    "    active_errors = []\n",
    "    \n",
    "    maxx = sum(np.max(np.array(old), axis=0))\n",
    "\n",
    "    for multi_index in active_set:\n",
    "        if (step > 0) and sum(np.max(np.array(old + [multi_index]), axis=0)) <= maxx:\n",
    "            active_set.remove(multi_index)\n",
    "        \n",
    "\n",
    "    for multi_index in active_set:\n",
    "        nodes, _ = build_nodes_weights(multi_index)\n",
    "        current_errors = []\n",
    "\n",
    "        for node in nodes:\n",
    "            a, b, d, g = node\n",
    "            if np.isnan(poly[-1](a, b, d, g)):\n",
    "                poly_eval = 0\n",
    "            else:\n",
    "                poly_eval = poly[-1](a, b, d, g)\n",
    "\n",
    "            if node in dick.keys():\n",
    "                a, b, d, g = node\n",
    "                current_errors.append(abs(dick[node] - poly_eval))\n",
    "\n",
    "            else:\n",
    "                solution = integrate.odeint(lotka, (33, 6.2), t, args=(a, b, d, g)).T[0][910]\n",
    "                current_errors.append(abs(solution - poly_eval))\n",
    "\n",
    "        active_errors.append(np.mean(current_errors))\n",
    "\n",
    "    active = sorted(list(zip(active_set, active_errors)), key=lambda x: x[1])\n",
    "\n",
    "    return active\n",
    "    # active = [i for i in OrderedDict((tuple(x[0]), x) for x in active).values()]if np.isnan(poly[-1](1,2,3,4)):\n",
    "\n",
    "def algorithm(P,species):\n",
    "    \n",
    "    global dick, old, candidates, poly, active, global_errors, no_nodes,step,expansion\n",
    "    \n",
    "    '''Initialise'''\n",
    "    \n",
    "    if species == 'prey':\n",
    "        target = 0\n",
    "        \n",
    "    elif species == 'predator':\n",
    "        target = 1\n",
    "    \n",
    "    seed = (2,2,1,2)\n",
    "    expansion = ch.generate_expansion(P, joint, normed = True)\n",
    "    exponents = ch.lead_exponent(expansion, graded=True)\n",
    "    vectors = np.identity(len(joint), dtype='int')\n",
    "    date_today = datetime.date.today()\n",
    "    start_time = time.perf_counter()\n",
    "    \n",
    "    step = 0\n",
    "    \n",
    "    old = [(0,0,0,0)]\n",
    "    active = []\n",
    "    poly = []\n",
    "    \n",
    "    local_errors = []\n",
    "    global_errors = []\n",
    "    \n",
    "    names = ['alpha','beta','delta','gamma']\n",
    "    \n",
    "    df = pd.DataFrame(columns=['chosen_index','local_error','global_error','no_nodes','run_time'],dtype=object)\n",
    "    df_indices = pd.DataFrame(columns=['alpha','beta','delta','gamma'],dtype=object)\n",
    "    df_indices_s1 = pd.DataFrame(columns=['alpha','beta','delta','gamma'],dtype=object)\n",
    "\n",
    "    \n",
    "    '''Execute zeroth step'''\n",
    "    \n",
    "    trivial = [seed]\n",
    "    number_nodes,uhat = solver(old,target)\n",
    "    assign_errors(old)\n",
    "    \n",
    "    \n",
    "    st = sense_t(uhat,exponents)\n",
    "    s1 = sense_main(uhat,exponents)\n",
    "    \n",
    "    global_errors.append(sobol_error(st))\n",
    "    \n",
    "    print('Global error >>>', global_errors[-1])\n",
    "    print('Step time >>>', time.perf_counter() - start_time, 'seconds')\n",
    "    print('-'*10,'break','-'*10)\n",
    "\n",
    "\n",
    "    '''Main loop'''\n",
    "    \n",
    "    while (global_errors[-1] > 0.2 or np.isnan(global_errors[-1])) and len(active)>0:\n",
    "        \n",
    "        start_time = time.perf_counter()\n",
    "        \n",
    "#         print('Active >>>',active)\n",
    "        chosen_index = active[-1][0]\n",
    "        local_errors.append(active[-1][1])\n",
    "        active.pop()\n",
    "    \n",
    "        old.append(chosen_index)\n",
    "        \n",
    "        print('Chosen index >>>', chosen_index)\n",
    "        \n",
    "        number_nodes,uhat = solver(old,target)\n",
    "        \n",
    "        candidates = generate_candidates(chosen_index,P)\n",
    "        stripped_active = [i[0] for i in active] + [j for j in candidates]\n",
    "        active = assign_errors(stripped_active)\n",
    "        \n",
    "        sobol_time = time.perf_counter() \n",
    "\n",
    "        st = sense_t(uhat,exponents)\n",
    "        s1 = sense_main(uhat,exponents)\n",
    "        \n",
    "        print('Sobol time >>>', time.perf_counter() - sobol_time)\n",
    "        \n",
    "        global_errors.append(sobol_error(st))\n",
    "\n",
    "        print('Global error >>>', global_errors[-1])\n",
    "        \n",
    "        '''Save data'''\n",
    "        run_time = time.perf_counter() - start_time\n",
    "        \n",
    "        numpoly.savez(f'../data/lotka2/{species}/poly_{P}+{date_today}.npz',*poly)\n",
    "        \n",
    "        df_indices = df_indices.append({'alpha': st[0], 'beta': st[1], 'delta': st[2], 'gamma': st[3]          }, ignore_index=True)\n",
    "        df_indices_s1 = df_indices_s1.append({'alpha': s1[0], 'beta': s1[1], 'delta': s1[2], 'gamma': s1[3]          }, ignore_index=True)\n",
    "        df = df.append({'chosen_index': chosen_index,'local_error':local_errors[-1],                               'global_error':global_errors[-1],'no_nodes':number_nodes, 'run_time':run_time}, ignore_index=True)\n",
    "        \n",
    "        df.to_csv(f'../data/lotka2/{species}/run_file_{P}+{date_today}.csv')\n",
    "        df_indices.to_csv(f'../data/lotka2/{species}/total_order_indices_{P}+{date_today}.csv')\n",
    "        df_indices_s1.to_csv(f'../data/lotka2/{species}/first_order_indices_{P}+{date_today}.csv')\n",
    "\n",
    "        print('Step time >>>', time.perf_counter() - start_time, 'seconds')\n",
    "        print('-'*10,'break','-'*10)\n",
    "        \n",
    "        step += 1\n",
    "        \n",
    "    print('Congratulations, the algorithm has converged!')\n",
    "    print('Here are the results...')\n",
    "    print('-'*20)\n",
    "    print(f'ST_alpha:{st[0].round(10)}, ST_beta:{st[1].round(10)}, ST_delta:{st[2].round(10)}, ST_gamma:{st[3].round(10)}')\n",
    "    print(f'GT_alpha:{gt_prey[0].round(10)}, GT_beta:{gt_prey[1].round(10)}, GT_delta:{gt_prey[2].round(10)}, GT_gamma:{gt_prey[3].round(10)}')\n",
    "    print(f'The final grid contains {number_nodes} nodes.')\n",
    "    print(f'The total run time was {df.run_time.sum()}seconds, not bad!')\n",
    "          \n",
    "\n",
    "def combinator(current_index):\n",
    "    \n",
    "    coeff = 1\n",
    "    \n",
    "    for vector in vectors:\n",
    "        \n",
    "        if tuple(np.array(current_index, dtype='int') + vector) in old:\n",
    "            \n",
    "            coeff += -1\n",
    "            \n",
    "    return coeff \n",
    "\n",
    "def build_nodes_weights(current_index):\n",
    "    \n",
    "    nodestack = []\n",
    "    weightstack = []\n",
    "    \n",
    "    '''Nodes'''\n",
    "    \n",
    "    for index,element in enumerate(current_index):\n",
    "        nodestack.append([])\n",
    "        nodestack[index] = list(x_lookup[index][element])\n",
    "        \n",
    "    nodes = nodestack[0]\n",
    "    \n",
    "    for i in range(1,len(nodestack)):\n",
    "        nodes = product(nodes,nodestack[i])\n",
    "        \n",
    "    nodes = [(a,b,c,d) for (((a,b),c),d) in nodes]\n",
    "    \n",
    "    '''Weights'''\n",
    "    \n",
    "    for index,element in enumerate(current_index):\n",
    "        weightstack.append([])\n",
    "        weightstack[index] = list(w_lookup[index][element])\n",
    "        \n",
    "    weights = weightstack[0]\n",
    "    \n",
    "    for i in range(1,len(weightstack)):\n",
    "        weights = product(weights,weightstack[i])\n",
    "        \n",
    "    weights = [(a*b*c*d) for (((a,b),c),d) in weights]\n",
    "    \n",
    "    \n",
    "    return nodes,weights\n",
    "\n",
    "def sense_main(uhat,exponents):\n",
    "\n",
    "    dim = len(joint)\n",
    "    s1 = np.zeros(dim)\n",
    "    \n",
    "    variance = np.sum(np.array(uhat[1:])**2)\n",
    "    \n",
    "    for variable,name in enumerate(expansion.names):\n",
    "        mask = np.ones(dim)\n",
    "        mask[variable] = False\n",
    "        \n",
    "        for idx,exponent in enumerate(exponents):\n",
    "            if exponent[variable] > 0 and np.all(exponent*mask == 0):\n",
    "                s1[variable] += uhat[idx]**2\n",
    "                \n",
    "    s1 = s1 / variance\n",
    "    \n",
    "    return s1\n",
    "\n",
    "def sense_t(uhat,exponents):\n",
    "    \n",
    "    dim = len(joint)\n",
    "    st = np.zeros(dim)\n",
    "    \n",
    "    variance = np.sum(np.array(uhat[1:])**2)\n",
    "    \n",
    "    for variable,name in enumerate(expansion.names):\n",
    "\n",
    "        mask = np.ones(dim)\n",
    "        mask[variable] = False\n",
    "        \n",
    "        for idx,exponent in enumerate(exponents):\n",
    "            if exponent[variable] > 0 and np.all(exponent*mask == 0):\n",
    "                st[variable] += uhat[idx]**2\n",
    "                \n",
    "            if exponent[variable] > 0 and np.any(exponent*mask != 0):\n",
    "                st[variable] += uhat[idx]**2\n",
    "                \n",
    "    st = st / variance\n",
    "    \n",
    "    return st\n",
    "    \n",
    "    \n",
    "        \n",
    "algorithm(3,'prey')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
